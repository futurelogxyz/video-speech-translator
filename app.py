import os
import sys
import gradio as gr
import datetime
from TTS.api import TTS
import subprocess
from faster_whisper import WhisperModel
import requests
from moviepy.editor import VideoFileClip

language_map = {
    "ar": "Arabic",
    "pt": "Brazilian Portuguese",
    "zh-cn": "Chinese",
    "cs": "Czech",
    "nl": "Dutch",
    "en": "English",
    "fr": "French",
    "de": "German",
    "it": "Italian",
    "pl": "Polish",
    "ru": "Russian",
    "es": "Spanish",
    "tr": "Turkish",
    "ja": "Japanese",
    "ko": "Korean",
    "hu": "Hungarian"
}

# get current file directory
current_file_dir = os.path.dirname(os.path.abspath(__file__))
video_length_seconds = 30

def get_video_length(video_path):
    video = VideoFileClip(video_path)
    return video.duration # è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰

# def get_audio_length(audio_path):
#     audio = AudioSegment.from_file(audio_path)
#     duration = len(audio) / 1000  # éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
#     return duration

def update_extract_end_time(video_path):
    if video_path is not None:
        return get_video_length(video_path)
    return 0

# def update_translated_speech_audio_speed(translated_speech_audio):
#     if translated_speech_audio is not None:
#         return round(get_audio_length(translated_speech_audio) / video_length_seconds, 2)
#     return 1.0

# time segment text example:
# [0.00s -> 0.50s]: Hello, my name is wallezen.
# [0.50s -> 1.00s]: I am a software engineer.
# [1.00s -> 1.50s]: I am from China.
def time_segment_text_to_srt(text):
    lines = text.strip().split('\n')
    srt_text = []
    pure_speech_text = []
    
    for i, line in enumerate(lines):
        timestamps, content = line.split("]: ")
        start_time, end_time = timestamps.strip("[").split(" -> ")
        start_time = start_time.replace("s", "").strip()
        end_time = end_time.replace("s", "").strip()
        
        srt_entry = []
        srt_entry.append(str(i + 1))

        srt_start_time = "00:00:0" + start_time.replace('.',',') if len(start_time.split('.')[0]) == 1 else "00:00:" + start_time.replace('.',',')
        srt_end_time = "00:00:0" + end_time.replace('.',',') if len(end_time.split('.')[0]) == 1 else "00:00:" + end_time.replace('.',',')
        srt_entry.append(f"{srt_start_time} --> {srt_end_time}")

        srt_entry.append(content)
        
        srt_text.append('\n'.join(srt_entry))

        pure_speech_text.append(content)
    
    return '\n\n'.join(srt_text), ' '.join(pure_speech_text)


def extract_audio_and_text(video_path, raw_speech_language, extract_start_time_seconds, extract_end_time_seconds):
    # ä¸Šä¼ è§†é¢‘ï¼Œæå–äººå£°å’Œæ–‡æœ¬
    if get_video_length(video_path) < 10:
        raise Exception("è§†é¢‘æ—¶é•¿é¡»å¤§äº 10 ç§’,ä¸è¶…è¿‡ 60 ç§’")

    video_file_name = video_path.split("/")[-1].split(".")[0]
    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

    raw_audio_file_name = f"{video_file_name}_{current_time}_{extract_start_time_seconds}_{extract_end_time_seconds}.wav"
    raw_audio_file_path = f"{current_file_dir}/output/raw_audio/{raw_audio_file_name}"

    raw_speech_path = f"{current_file_dir}/output/raw_speech/"
    raw_speech_file_path = f"{raw_speech_path}/{video_file_name}_{current_time}_{extract_start_time_seconds}_{extract_end_time_seconds}/vocals.wav"
    raw_accompaniment_file_path = f"{raw_speech_path}/{video_file_name}_{current_time}_{extract_start_time_seconds}_{extract_end_time_seconds}/accompaniment.wav"

    ## step 1. use ffmpeg to extract audio from video
    duration = extract_end_time_seconds - extract_start_time_seconds
    if duration <= 0:
        raise Exception("æå–æ—¶é—´é”™è¯¯ï¼Œç»“æŸæ—¶é—´å¿…é¡»å¤§äºèµ·å§‹æ—¶é—´")
    if duration > 60:
        raise Exception("æå–æ—¶é—´é”™è¯¯ï¼Œæœ€é•¿æå–60ç§’")

    extract_raw_audio_cmd = [
        "ffmpeg", 
        "-i", f"{video_path}", 
        "-ss", f"00:00:{extract_start_time_seconds}", 
        "-t", f"00:00:{duration}", 
        "-vn", 
        "-acodec", "pcm_s16le", 
        "-ar", "44100", 
        "-ac", "2", 
        f"{raw_audio_file_path}"]
    print(" ".join(extract_raw_audio_cmd))
    result = subprocess.run(extract_raw_audio_cmd, capture_output=True, text=True)
    print(result)
    # To check if the command was successful
    if result.returncode == 0:
        print("ä»è§†é¢‘ä¸­æå–éŸ³é¢‘æˆåŠŸ")
    else:
        raise Exception("ä»è§†é¢‘ä¸­æå–éŸ³é¢‘å¤±è´¥")

    ## step 2. use spleeter to extract speech from audio
    spleeter_cmd_env = os.path.join(os.environ.get("CONDA_VIRTUAL_ENV_PATH"), "video-speech-localization-spleeter")
    extract_speech_cmd = [
        "conda", 
        "run", 
        "-p", f"{spleeter_cmd_env}", 
        "spleeter", 
        "separate", 
        "-p", "spleeter:2stems", 
        "-o", f"{raw_speech_path}", 
        f"{raw_audio_file_path}"]
    print(" ".join(extract_speech_cmd))
    result = subprocess.run(extract_speech_cmd, capture_output=True, text=True)
    print(result)
    # To check if the command was successful
    if result.returncode == 0:
        print("ä»éŸ³é¢‘ä¸­æå–äººå£°æˆåŠŸ")
    else:
        raise Exception("ä»éŸ³é¢‘ä¸­æå–äººå£°å¤±è´¥")

    whisper_model = "/data/.hugggingface/cache/hub/models--guillaumekln--faster-whisper-large-v2/refs/main"
    ## step 3. call faster-whisper to recognize speech transcript from speech
    # Run on GPU with FP16
    # model = WhisperModel("large-v2", device="cuda", compute_type="float16")
    # or run on GPU with INT8
    # model = WhisperModel("large-v2", device="cuda", compute_type="int8_float16")
    # or run on CPU with INT8
    model = WhisperModel(whisper_model, device="cpu", compute_type="int8")

    segments, info = model.transcribe(f"{raw_speech_file_path}", beam_size=5)

    print("Detected language '%s' with probability %f" % (info.language, info.language_probability))

    raw_speech_text = []
    raw_speech_text_segment = []
    for segment in segments:
        tmp = "[%.2fs -> %.2fs]: %s" % (segment.start, segment.end, segment.text)
        print(tmp)
        raw_speech_text.append(segment.text)  # TODO: reserve start and end time of every segment in order to align with original video
        raw_speech_text_segment.append(tmp)

    raw_speech_text = " ".join(raw_speech_text)
    raw_speech_text_segment = "\n".join(raw_speech_text_segment)

    return raw_speech_file_path, raw_accompaniment_file_path, raw_speech_text, raw_speech_text_segment


def translate(raw_speech_audio, raw_speech_text, raw_speech_text_segment, target_language):
    # ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€
    ## step 1. call chatGPT to translate speech text
    prompt = f"""
    Translate the following text to {language_map[target_language]}, add missing punctuation marks, preserving the format, not translate words between < and >, not include any other instructions in response:\n\n
    {raw_speech_text}
    """

    # request OpenAI API using OpenAI python client
    # client = OpenAI()
    # response = client.chat.completions.create(
    #     model="gpt-3.5-turbo",
    #     messages=[
    #         {"role": "system", "content": "You are a helpful translator."},
    #         {"role": "user", "content": prompt}
    #     ]
    # )
    # translated_speech_text = response.choices[0].message.content

    # request OpenAI API using requests
    openai_chat_api_url = os.environ.get("OPENAI_CHAT_API_URL")
    headers = {
        "Authorization": os.environ.get("OPENAI_API_KEY"),  # FIXME: change this to "Bear <YOUR_OPENAI_API_KEY>" if you want to request OpenAI API directly
        "Content-Type": "application/json"
    }

    payload = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": "You are a helpful translator for localizing advertisement creatives."},
            {"role": "user", "content": f"{prompt}"}
        ]
    }

    translated_speech_text = ""
    try:
        with requests.post(openai_chat_api_url, headers=headers, json=payload) as response:
            if response.status_code == 200:
                response_data = response.json()
                print(response_data)
                translated_speech_text = response_data["choices"][0]["message"]["content"]
            else:
                # print("è¯·æ±‚ iGateway OpenAI Chat æ¥å£å‡ºé”™: ", response)
                raise Exception("è¯·æ±‚ iGateway OpenAI Chat æ¥å£å‡ºé”™: ", response)
    except requests.exceptions.RequestException as e:
        # print("request openai failed:", e)
        raise Exception("è¯·æ±‚ iGateway OpenAI Chat æ¥å£å¤±è´¥: ", e)

    if translated_speech_text == "":
        raise Exception("è¯·æ±‚ iGateway OpenAI ç¿»è¯‘å¤±è´¥")

    ## step 2. call chatGPT to translate speech text segment
    prompt = f"""
    Translate the following text to {language_map[target_language]}, add missing punctuation marks, preserving the format, not translate words between < and >, not include any other instructions in response:\n\n
    {raw_speech_text_segment}
    """

    payload = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": "You are a helpful translator for localizing advertisement creatives."},
            {"role": "user", "content": f"{prompt}"}
        ]
    }

    translated_speech_text_segment = ""
    try:
        with requests.post(openai_chat_api_url, headers=headers, json=payload) as response:
            if response.status_code == 200:
                response_data = response.json()
                print(response_data)
                translated_speech_text_segment = response_data["choices"][0]["message"]["content"]
            else:
                # print("è¯·æ±‚ iGateway OpenAI Chat æ¥å£å‡ºé”™: ", response)
                raise Exception("è¯·æ±‚ iGateway OpenAI Chat æ¥å£å‡ºé”™: ", response)
    except requests.exceptions.RequestException as e:
        # print("request openai failed:", e)
        raise Exception("è¯·æ±‚ iGateway OpenAI Chat æ¥å£å¤±è´¥: ", e)

    if translated_speech_text_segment == "":
        raise Exception("è¯·æ±‚ iGateway OpenAI ç¿»è¯‘æŒ‰æ—¶é—´åˆ†æ®µæ–‡æœ¬å¤±è´¥")

    return translated_speech_text, translated_speech_text_segment


def compose_target_language_audio(raw_speech_audio, translated_speech_text, translated_speech_text_segment, target_language, speech_speed):
    # åˆæˆç›®æ ‡è¯­è¨€äººå£°
    ## use coqui-xTTS-V2 to synthesize target language speech audio and clone raw speech tone
    raw_speech_file_name = raw_speech_audio.split("/")[-2] + "_" + raw_speech_audio.split("/")[-1].split(".")[0]
    translated_speech_file_path = f"{current_file_dir}/output/translated_speech/{target_language}_{raw_speech_file_name}.wav"
    translated_speech_srt_file_path = f"{current_file_dir}/output/translated_speech/{target_language}_{raw_speech_file_name}.srt"

    # write translated speech text to srt file
    srt_text, pure_speech_text = time_segment_text_to_srt(translated_speech_text_segment)
    print(pure_speech_text)
    with open(translated_speech_srt_file_path, "w", encoding="utf-8") as f:
        f.write(srt_text)

    # use TTS to synthesize target language speech audio
    # Init coqui ğŸ¸TTS
    tts = TTS(
        #model_name="tts_models/multilingual/multi-dataset/xtts_v2",
        model_path="/root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2/",
        config_path="/root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2/config.json"
    ).to("cuda")

    # Run TTS
    # â— Since xtts_v2 model is multi-lingual voice cloning model, we must set the target speaker_wav and language
    # Text to speech list of amplitude values as output
    # wav = tts.tts(text="Hello world!", speaker_wav="my/cloning/audio.wav", language="en")
    # Text to speech to a file
    tts.tts_to_file(text=f"{pure_speech_text}", speaker_wav=f"{raw_speech_audio}", language=f"{target_language}", file_path=f"{translated_speech_file_path}", speed=speech_speed)

    return translated_speech_file_path, translated_speech_srt_file_path


def compose_lip_sync_video(original_video, translated_speech_audio):
    # åˆæˆå£å‹å¯¹é½è§†é¢‘
    ## use video-retalking to generate lip-synced video
    lip_sync_video_file_path = f"{current_file_dir}/output/lip_synced_video/{original_video.split('/')[-1].split('.')[0]}-{translated_speech_audio.split('/')[-1].split('.')[0]}.mp4"

    video_retalking_env = os.path.join(os.environ.get("CONDA_VIRTUAL_ENV_PATH"), "video-speech-localization-video-retalking")
    compose_lip_sync_video_cmd = [
        # "export", "CUDA_HOME=/usr/local/cuda-11.8/",
        # "&&",
        # "export", "CUDA_VISIBLE_DEVICES=4",
        # "&&",
        # "cd", f"{current_file_dir}/video-retalking", 
        # "&&",
        "conda", 
        "run", 
        "-p",  f"{video_retalking_env}", 
        "python",
        "inference.py",
        "--face",  f"{original_video}",
        "--audio", f"{translated_speech_audio}",
        "--outfile", f"{lip_sync_video_file_path}"
    ]
    video_retalking_env_vars = {
        "CUDA_HOME": os.environ.get("CUDA_HOME"),
        "CUDA_VISIBLE_DEVICES": os.environ.get("CUDA_VISIBLE_DEVICES"),
        "LD_LIBRARY_PATH": os.environ.get("LD_LIBRARY_PATH")
    }
    video_retalking_workdir = f"{current_file_dir}/video-retalking"

    print(" ".join(compose_lip_sync_video_cmd))
    result = subprocess.run(compose_lip_sync_video_cmd, capture_output=True, text=True, cwd=video_retalking_workdir, env=video_retalking_env_vars)
    print(result)
    # To check if the command was successful
    if result.returncode == 0:
        print("åˆæˆå£å‹å¯¹é½è§†é¢‘æˆåŠŸ")
    else:
        raise Exception("åˆæˆå£å‹å¯¹é½è§†é¢‘å¤±è´¥") 

    return lip_sync_video_file_path


def compose_final_video_without_lip_sync(original_video, translated_speech_audio, raw_accompaniment_audio, translated_speech_srt):
    # åˆæˆæœ€ç»ˆè§†é¢‘
    finale_video_file_path = f"{current_file_dir}/output/final_video/{translated_speech_audio.split('/')[-1].split('.')[0]}.mp4"
    ## use ffmpeg to replace original video speech with translated speech, and mix with raw accompaniment audio
    compose_cmd = [
        "ffmpeg", 
        "-i", f"{original_video}", 
        "-i", f"{translated_speech_audio}", 
        "-i", f"{raw_accompaniment_audio}", 
        "-vf", f"subtitles={translated_speech_srt},drawtext=text='wallezen@CrowAI':x=10:y=10:fontsize=24:fontcolor=white:fontfile=/usr/share/fonts/dejavu/DejaVuSansMono.ttf",
        "-filter_complex", "[1:a]atempo=1.0[a1];[2:a]atempo=1.0[a2];[a1][a2]amix=inputs=2[a]", 
        "-map", "0:v",
        "-map", "[a]",
        "-c:a", "aac",
        "-ac", "2",
        f"{finale_video_file_path}"
    ]
    print(" ".join(compose_cmd))
    result = subprocess.run(compose_cmd, capture_output=True, text=True)
    print(result)
    # To check if the command was successful
    if result.returncode == 0:
        print("åˆæˆæœ€ç»ˆè§†é¢‘æˆåŠŸ")
    else:
        raise Exception("åˆæˆæœ€ç»ˆè§†é¢‘å¤±è´¥")

    return finale_video_file_path


def compose_final_video_with_lip_sync(lip_sync_video, raw_accompaniment_audio, translated_speech_srt):
    # åˆæˆæœ€ç»ˆè§†é¢‘
    finale_video_file_path = f"{current_file_dir}/output/final_video/{lip_sync_video.split('/')[-1].split('.')[0]}.mp4"
    ## use ffmpeg to replace original video speech with translated speech
    compose_cmd = [
        "ffmpeg", 
        "-i", f"{lip_sync_video}", 
        "-i", f"{raw_accompaniment_audio}", 
        "-vf", f"subtitles={translated_speech_srt},drawtext=text='wallezen@CrowAI':x=10:y=10:fontsize=24:fontcolor=white:fontfile=/usr/share/fonts/dejavu/DejaVuSansMono.ttf",
        "-filter_complex", "[0:a][1:a]amerge=inputs=2[a]",
        "-map", "0:v",
        "-map", "[a]",
        "-c:a", "aac",
        "-ac", "2",
        f"{finale_video_file_path}"]
    print(" ".join(compose_cmd))
    result = subprocess.run(compose_cmd, capture_output=True, text=True)
    print(result)
    # To check if the command was successful
    if result.returncode == 0:
        print("åˆæˆæœ€ç»ˆè§†é¢‘æˆåŠŸ")
    else:
        raise Exception("åˆæˆæœ€ç»ˆè§†é¢‘å¤±è´¥")

    return finale_video_file_path

def compose_final_video(original_video, translated_speech_audio, raw_accompaniment_audio, translated_speech_srt, lip_sync_radio, subtitle_radio):
    # compose final video
    # step 1. compose lip-synced video
    lip_sync_video = ""
    if lip_sync_radio == "æ˜¯":
        lip_sync_video = compose_lip_sync_video(original_video, translated_speech_audio)

    finale_video_file_path = ""
    if lip_sync_video != "":
        finale_video_file_path = compose_final_video_with_lip_sync(lip_sync_video, raw_accompaniment_audio, translated_speech_srt)
    else:
        finale_video_file_path = compose_final_video_without_lip_sync(original_video, translated_speech_audio, raw_accompaniment_audio, translated_speech_srt)

    if finale_video_file_path == "":
        raise Exception("åˆæˆæœ€ç»ˆè§†é¢‘å¤±è´¥")

    return finale_video_file_path

with gr.Blocks() as app:
    gr.Markdown("## è§†é¢‘æœ¬åœ°åŒ– Demo [Github](https://github.com/crowaixyz/video-speech-localization)")
    # step 1. ä¸Šä¼ è§†é¢‘
    gr.Markdown("### Step 1. ä¸Šä¼ è§†é¢‘")
    with gr.Row():
        original_video = gr.Video(label="åŸå§‹è§†é¢‘(æ³¨æ„è§†é¢‘æ—¶é•¿é¡»å¤§äº10ç§’ï¼Œä¸è¶…è¿‡60ç§’, 30~60ç§’ä¸ºä½³)")
        with gr.Column():
            original_videl_speech_language = gr.Dropdown(choices=["ar","pt","zh-cn","cs","nl","en","fr","de","it","pl","ru","es","tr","ja","ko","hu"], label="è§†é¢‘äººå£°è¯­è¨€")
            gr.Markdown("ar:    Arabic <br />pt: Brazilian    Portuguese <br />zh-cn: Chinese <br />cs:    Czech <br />nl:    Dutch <br />en:    English <br />fr:    French <br />de:    German <br />it:    Italian <br />pl:    Polish <br />ru:    Russian <br />es:    Spanish <br />tr:    Turkish <br />ja:    Japanese <br />ko:    Korean <br />hu: Hungarian")

    # step 2. æå–äººå£°ï¼Œæ–‡æœ¬å’ŒèƒŒæ™¯éŸ³ä¹
    gr.Markdown("### Step 2. æå–äººå£°ï¼Œæ–‡æœ¬å’ŒèƒŒæ™¯éŸ³ä¹")
    with gr.Row():
        with gr.Column():
            extract_start_time_seconds = gr.Slider(label="æå–èµ·å§‹æ—¶é—´(ç§’)", minimum=0.0, maximum=60.0, value=0.0, interactive=False)
            extract_end_time_seconds = gr.Slider(label="æå–ç»“æŸæ—¶é—´(ç§’)", minimum=0.0, maximum=60.0, value=30.0, interactive=False)

        audio_extract_button = gr.Button("ç‚¹å‡»æå–")

    raw_speech_audio = gr.Audio(label="äººå£°", type="filepath", interactive=False)
    raw_accompaniment_audio = gr.Audio(label="èƒŒæ™¯éŸ³ä¹", type="filepath", interactive=False)
    raw_speech_text_segment = gr.Textbox(label="æŒ‰æ—¶é—´åˆ†æ®µçš„äººå£°æ–‡æœ¬ï¼ˆå¯ä¿®æ”¹ï¼Œå¯¹äºä¸éœ€è¦è¿›è¡Œåç»­ç¿»è¯‘çš„è¯ä½¿ç”¨'<>'ï¼‰", interactive=True)
    raw_speech_text = gr.Textbox(label="å®Œæ•´ä¸åˆ†æ®µçš„äººå£°æ–‡æœ¬")

    # step 3. ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€
    gr.Markdown("### Step 3. ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€")
    with gr.Row():
        target_speech_language = gr.Dropdown(choices=["ar","pt","zh-cn","cs","nl","en","fr","de","it","pl","ru","es","tr","ja","ko","hu"], label="é€‰æ‹©ç›®æ ‡è¯­è¨€")
        translate_button = gr.Button("ç‚¹å‡»ç¿»è¯‘")

    translated_speech_text_segment = gr.Textbox(label="ç¿»è¯‘åçš„æŒ‰æ—¶é—´åˆ†æ®µçš„äººå£°æ–‡æœ¬(å¯ä¿®æ”¹)", interactive=True)
    translated_speech_text = gr.Textbox(label="ç¿»è¯‘åçš„å®Œæ•´ä¸åˆ†æ®µçš„äººå£°æ–‡æœ¬", interactive=False)

    # step 4. åˆæˆç›®æ ‡è¯­è¨€äººå£°
    gr.Markdown("### Step 4. åˆæˆç›®æ ‡è¯­è¨€äººå£°")
    with gr.Row():
        speech_speed = gr.Slider(label="è°ƒæ•´äººå£°è¯­é€Ÿ", minimum=0.5, maximum=2.0, value=1.0, step=0.01, interactive=True, info="é€šè¿‡è°ƒæ•´è¯­é€Ÿï¼Œå°½é‡ä¿è¯ç”Ÿæˆçš„è¯­éŸ³å’ŒåŸè§†é¢‘æ—¶é•¿ä¸€è‡´")
        compose_target_language_audio_button = gr.Button("ç‚¹å‡»åˆæˆ")

    translated_speech_audio = gr.Audio(label="ç›®æ ‡è¯­è¨€äººå£°", type="filepath", interactive=False)
    translated_speech_srt = gr.File(label="ç›®æ ‡è¯­è¨€å­—å¹• SRT", type="filepath", interactive=False)

    # Step 5. åˆæˆæœ€ç»ˆè§†é¢‘
    gr.Markdown("### Step 5. åˆæˆæœ€ç»ˆè§†é¢‘")
    with gr.Row():
        with gr.Column():
            lip_sync_radio = gr.Radio(choices=["æ˜¯", "å¦"], label="æ˜¯å¦å¯¹é½å£å‹", value="å¦", interactive=True, info="ä»…é€‚åˆå•äººè¯´è¯åœºæ™¯ï¼Œéœ€è¦å§‹ç»ˆä¿æŒéœ²è„¸ï¼Œä¸”è€—æ—¶è¾ƒé•¿ï¼")
            subtitle_radio = gr.Radio(choices=["æ˜¯", "å¦"], label="æ˜¯å¦æ·»åŠ å­—å¹•", value="æ˜¯", interactive=False, info="ç›®æ ‡è¯­è¨€å­—å¹•")
        compose_final_video_button = gr.Button("ç‚¹å‡»åˆæˆ")
    with gr.Row():
        final_video = gr.Video(label="æœ€ç»ˆè§†é¢‘", interactive=False)

    # å›è°ƒäº‹ä»¶
    ## ä¸Šä¼ è§†é¢‘åï¼Œè‡ªåŠ¨è·å–è§†é¢‘æ—¶é•¿ï¼Œå¹¶æ›´æ–°æå–ç»“æŸæ—¶é—´ slider ç»„ä»¶çš„å€¼
    original_video.change(fn=update_extract_end_time, inputs=original_video, outputs=extract_end_time_seconds)
    
    # å¤„ç†æŒ‰é’®ç‚¹å‡»äº‹ä»¶
    audio_extract_button.click(
        extract_audio_and_text,
        inputs=[original_video, original_videl_speech_language, extract_start_time_seconds, extract_end_time_seconds],
        outputs=[raw_speech_audio, raw_accompaniment_audio, raw_speech_text, raw_speech_text_segment]
    )
    translate_button.click(
        translate,
        inputs=[raw_speech_audio, raw_speech_text, raw_speech_text_segment, target_speech_language],
        outputs=[translated_speech_text, translated_speech_text_segment]
    )
    compose_target_language_audio_button.click(
        compose_target_language_audio,
        inputs=[raw_speech_audio, translated_speech_text, translated_speech_text_segment, target_speech_language, speech_speed],
        outputs=[translated_speech_audio, translated_speech_srt]
    )
    compose_final_video_button.click(
        compose_final_video,
        inputs=[original_video, translated_speech_audio, raw_accompaniment_audio, translated_speech_srt, lip_sync_radio, subtitle_radio],
        outputs=[final_video]
    )

app.launch(server_name=os.environ.get("VSL_SERVER_NAME"), server_port=int(os.environ.get("VSL_SERVER_PORT")), share=False)
